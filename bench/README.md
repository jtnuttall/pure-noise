# `pure-noise` benchmarks

## Summary

These benchmarks measure single-threaded and parallel noise generation performance for `pure-noise`,
a pure Haskell noise library compiled with the LLVM backend. The headline result: **pure Haskell achieves
84-95% of C++ FastNoiseLite performance** in single-threaded scenarios, with simpler noise algorithms
(Perlin, OpenSimplex2, SuperSimplex, Cellular) reaching 86-95% of C++ speed.

**Hardware:** i9-13900K, Fedora 42 distrobox, LLVM 15

**Parallel performance** using `massiv` reaches over **1.7 billion values/sec** for 2D Perlin noise,
demonstrating 10-15× speedups over single-threaded execution.

## Comparison with FastNoiseLite

To provide an accurate, hardware-equivalent comparison, benchmarks for [FastNoiseLite (FNL)](https://github.com/Auburn/FastNoiseLite)
were run locally on the same machine using the `NoiseBenchmarking` tool and compared against this library's
results (compiled with the LLVM backend).

### Single-Thread Performance Comparison

**Direct comparison (integer-aligned grid, 512×512 for 2D, 64³ for 3D):**

| Noise Type          | pure-noise (Float) | FastNoiseLite | % of FNL |
| :------------------ | :----------------- | :------------ | :------- |
| **Cellular 2D**     | 68_186_004         | 71_874_200    | 94.9%    |
| **SuperSimplex 2D** | 98_128_312         | 106_499_000   | 92.1%    |
| **Perlin 3D**       | 79_949_212         | 90_821_200    | 88.0%    |
| **OpenSimplex2 2D** | 116_801_665        | 133_444_000   | 87.5%    |
| **Value 3D**        | 96_972_157         | 111_514_000   | 87.0%    |
| **Perlin 2D**       | 158_245_846        | 182_558_000   | 86.7%    |
| **ValueCubic 2D**   | 62_931_346         | 73_415_900    | 85.7%    |
| **Value 2D**        | 178_258_969        | 211_971_000   | 84.1%    |
| **ValueCubic 3D**   | 19_674_751         | 23_361_200    | 84.2%    |

Pure Haskell with LLVM achieves **84-95% of C++ performance** in single-threaded scenarios.

### Parallel Performance

The `massiv`-based parallel benchmarks demonstrate significantly higher throughput by leveraging multiple cores.
This is the intended path for high-performance, large-scale noise generation with this library.

For example, the parallel `massiv` 2D Perlin benchmark achieves **~1.73 billion values/sec**, over 9× the
single-threaded FNL result and 11× the single-threaded pure-noise result.

## Detailed `pure-noise` Results (LLVM)

Measured by values per second (vps) generated by the noise functions. These results are from benchmarks
run with the `-fllvm` backend.

### 2D (Single-Thread)

| name          | Float (vps) | Double (vps) |
| :------------ | :---------- | :----------- |
| value2        | 173_511_654 | 189_119_731  |
| perlin2       | 154_674_464 | 161_114_532  |
| openSimplex2  | 74_747_031  | 74_332_345   |
| valueCubic2   | 61_415_544  | 62_481_313   |
| superSimplex2 | 51_295_369  | 50_383_577   |
| cellular2     | 34_996_382  | 32_652_899   |

### 3D (Single-Thread)

| name        | Float (vps) | Double (vps) |
| :---------- | :---------- | :----------- |
| value3      | 90_805_572  | 93_188_363   |
| perlin3     | 74_080_032  | 82_477_882   |
| valueCubic3 | 18_765_912  | 18_284_749   |

### 2D Parallel (`massiv`)

| name          | Float (vps)   | Double (vps)  |
| :------------ | :------------ | :------------ |
| value2        | 2_349_788_931 | 2_174_853_762 |
| perlin2       | 1_733_510_111 | 1_456_539_691 |
| openSimplex2  | 1_126_706_858 | 985_365_752   |
| valueCubic2   | 1_100_260_007 | 1_038_308_851 |
| superSimplex2 | 819_495_064   | 792_605_815   |
| cellular2     | 608_799_300   | 585_049_209   |

## Methodology & Reproducibility

### Measurement Approach

Benchmarks are run by mapping a noise function over a 1 million element unboxed array of indices:

- This creates approximately 1-2ms of overhead
- Using index tuples increases the probability of hitting diverse code paths in the noise implementation.
  Some noise functions may skip certain computations when specific conditions are met relative to the input.
- Memory allocation is constant:
  - Float ~= 4.0MB (4 bytes × 1_000_000 elements)
  - Double ~= 8.0MB (8 bytes × 1_000_000 elements)

Benchmarks that use `massiv` demonstrate thread-level parallelism and are the intended path of use for
most purposes.

### Two Benchmark Approaches

Two benchmark approaches are used to balance real-world accuracy and fair comparison:

1. **Standard benchmarks** (1M values): Use fractional coordinates with random offsets to exercise all
   code paths and avoid potential fast-path optimizations at integer boundaries. These are more representative
   of typical use cases.

2. **FNL-comparison benchmarks** (262K values): Use integer-aligned grid coordinates (512×512 for 2D,
   64³ for 3D) matching FNL's exact methodology for direct apples-to-apples comparison. These benchmarks
   use `foldl'` with summation to prevent dead code elimination, matching FNL's `DoNotOptimize` approach.
   This adds ~2-5% overhead from coordinate tuple loading and floating-point addition, but provides a
   fairer comparison than materializing output vectors.

### Coordinate Pattern Impact

The choice of coordinate pattern affects performance differently across algorithms:

- **Cellular**: Integer coordinates provide +10-15% performance improvement over fractional coordinates.
  Distance-based algorithms benefit from simplified calculations and improved branch prediction at grid
  boundaries where cells align exactly.
- **OpenSimplex2/SuperSimplex**: Integer coordinates are significantly faster (+47% to +83%) than fractional,
  likely due to improved cache locality and branch prediction with regular grid patterns.
- **Perlin/Value/ValueCubic**: Integer coordinates are slightly slower (-1 to -2%) than fractional.

This variation demonstrates that different noise algorithms have fundamentally different performance
characteristics depending on coordinate access patterns. The FNL comparison benchmarks use integer grids
(matching FNL's methodology), while standard benchmarks use fractional coordinates (more representative
of typical use cases).

### Reproducibility Scripts

There are two scripts in this folder for running benchmarks with the exact parameters used to generate
these results:

- `results/collect-bench.sh` — Documents the parameters used to create benchmark results
- `results/vps.py` — Calculates values/second numbers from `tasty-bench` CSV output
